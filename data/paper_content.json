{"https://arxiv.org/abs/2509.01432": {"results": [{"url": "https://arxiv.org/abs/2509.01432", "title": "The Geometry of Nonlinear Reinforcement Learning", "raw_content": "[2509.01432] The Geometry of Nonlinear Reinforcement Learning\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\n\n>cs> arXiv:2509.01432\nHelp | Advanced Search\nSearch\n\nGO\nquick links\n\nLogin\nHelp Pages\nAbout\n\nComputer Science > Machine Learning\narXiv:2509.01432 (cs)\n[Submitted on 1 Sep 2025]\nTitle:The Geometry of Nonlinear Reinforcement Learning\nAuthors:Nikola Milosevic, Nico Scherf\nView a PDF of the paper titled The Geometry of Nonlinear Reinforcement Learning, by Nikola Milosevic and Nico Scherf\nView PDFHTML (experimental)\n\nAbstract:Reward maximization, safe exploration, and intrinsic motivation are often studied as separate objectives in reinforcement learning (RL). We present a unified geometric framework, that views these goals as instances of a single optimization problem on the space of achievable long-term behavior in an environment. Within this framework, classical methods such as policy mirror descent, natural policy gradient, and trust-region algorithms naturally generalize to nonlinear utilities and convex constraints. We illustrate how this perspective captures robustness, safety, exploration, and diversity objectives, and outline open challenges at the interface of geometry and deep RL.\n\nSubjects:Machine Learning (cs.LG)\nCite as:arXiv:2509.01432 [cs.LG]\n(or arXiv:2509.01432v1 [cs.LG] for this version)\n\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Nikola Milosevic [view email]\n[v1] Mon, 1 Sep 2025 12:42:43 UTC (732 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled The Geometry of Nonlinear Reinforcement Learning, by Nikola Milosevic and Nico Scherf\n\nView PDF\nHTML (experimental)\nTeX Source\n\nview license\nCurrent browse context:\ncs.LG\n<prev | next>\nnew | recent | 2025-09\nChange to browse by:\ncs\nReferences & Citations\n\nNASA ADS\nGoogle Scholar\nSemantic Scholar\n\nexport BibTeX citation Loading...\nBibTeX formatted citation\n\u00d7\nData provided by: \nBookmark\n\nBibliographic Tools\nBibliographic and Citation Tools\n\n[x] Bibliographic Explorer Toggle\n\nBibliographic Explorer (What is the Explorer?)\n\n[x] Connected Papers Toggle\n\nConnected Papers (What is Connected Papers?)\n\n[x] Litmaps Toggle\n\nLitmaps (What is Litmaps?)\n\n[x] scite.ai Toggle\n\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nCode, Data and Media Associated with this Article\n\n[x] alphaXiv Toggle\n\nalphaXiv (What is alphaXiv?)\n\n[x] Links to Code Toggle\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\n[x] DagsHub Toggle\n\nDagsHub (What is DagsHub?)\n\n[x] GotitPub Toggle\n\nGotit.pub (What is GotitPub?)\n\n[x] Huggingface Toggle\n\nHugging Face (What is Huggingface?)\n\n[x] Links to Code Toggle\n\nPapers with Code (What is Papers with Code?)\n\n[x] ScienceCast Toggle\n\nScienceCast (What is ScienceCast?)\nDemos\nDemos\n\n[x] Replicate Toggle\n\nReplicate (What is Replicate?)\n\n[x] Spaces Toggle\n\nHugging Face Spaces (What is Spaces?)\n\n[x] Spaces Toggle\n\nTXYZ.AI (What is TXYZ.AI?)\nRelated Papers\nRecommenders and Search Tools\n\n[x] Link to Influence Flower\n\nInfluence Flower (What are Influence Flowers?)\n\n[x] Core recommender toggle\n\nCORE Recommender (What is CORE?)\n\n[x] IArxiv recommender toggle\n\nIArxiv Recommender (What is IArxiv?)\n\nAuthor\nVenue\nInstitution\nTopic\n\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax) (What is MathJax?)\n\nAbout\n\nHelp\n\nContact\n\nSubscribe\n\nCopyright\n\nPrivacy Policy\n\nWeb Accessibility Assistance\n\narXiv Operational Status", "images": []}], "failed_results": [], "response_time": 3.0, "request_id": "5e3505c6-2700-4f8d-b84c-53a32503ea11"}, "https://arxiv.org/abs/2508.18420": {"results": [{"url": "https://arxiv.org/abs/2508.18420", "title": "[2508.18420] LLM-Driven Intrinsic Motivation for Sparse Reward ...", "raw_content": "We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\n\ncs > arXiv:2508.18420\n\nComputer Science > Machine Learning\narXiv:2508.18420 (cs)\n[Submitted on 25 Aug 2025]\nTitle:LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning\nAuthors:Andr\u00e9 Quadros, Cassio Silva, Ronnie Alves\nView a PDF of the paper titled LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning, by Andr'e Quadros and 1 other authors\nView PDF HTML (experimental)\n\nAbstract:This paper explores the combination of two intrinsic motivation strategies to improve the efficiency of reinforcement learning (RL) agents in environments with extreme sparse rewards, where traditional learning struggles due to infrequent positive feedback. We propose integrating Variational State as Intrinsic Reward (VSIMR), which uses Variational AutoEncoders (VAEs) to reward state novelty, with an intrinsic reward approach derived from Large Language Models (LLMs). The LLMs leverage their pre-trained knowledge to generate reward signals based on environment and goal descriptions, guiding the agent. We implemented this combined approach with an Actor-Critic (A2C) agent in the MiniGrid DoorKey environment, a benchmark for sparse rewards. Our empirical results show that this combined strategy significantly increases agent performance and sampling efficiency compared to using each strategy individually or a standard A2C agent, which failed to learn. Analysis of learning curves indicates that the combination effectively complements different aspects of the environment and task: VSIMR drives exploration of new states, while the LLM-derived rewards facilitate progressive exploitation towards goals.\n\nComments:\n11 pages, 5 figures, Accepted to the ENIAC 2025 conference\n\nSubjects:\nMachine Learning (cs.LG)\n\nACM classes:\nI.2.6\n\nCite as:\narXiv:2508.18420 [cs.LG]\n\n(or  arXiv:2508.18420v1 [cs.LG] for this version)\n\n arXiv-issued DOI via DataCite\n\nSubmission history\nFrom: Andr\u00e9 Quadros [view email]\n[v1] Mon, 25 Aug 2025 19:10:58 UTC (469 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning, by Andr'e Quadros and 1 other authors\n\nView PDF\nHTML (experimental)\nTeX Source\n\nview license\nCurrent browse context:\ncs.LG\n< prev    |    next >\nnew  |  recent  | 2025-08\nChange to browse by:\ncs\nReferences & Citations\n\nNASA ADS\nGoogle Scholar\nSemantic Scholar\n\nexport BibTeX citation Loading...\nBibTeX formatted citation\n\u00d7\nData provided by:\nBookmark\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nReplicate (What is Replicate?)\nHugging Face Spaces (What is Spaces?)\nTXYZ.AI (What is TXYZ.AI?)\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\nIArxiv Recommender (What is IArxiv?)\n\nAuthor\nVenue\nInstitution\nTopic\n\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax) (What is MathJax?)", "images": []}], "failed_results": [], "response_time": 0.01, "request_id": "3124ee14-91d2-4e04-95dd-aa23a70923d8"}, "https://arxiv.org/abs/2503.21047": {"results": [{"url": "https://arxiv.org/abs/2503.21047", "title": "World Model Agents with Change-Based Intrinsic Motivation", "raw_content": "[2503.21047] World Model Agents with Change-Based Intrinsic Motivation\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\n\n>cs> arXiv:2503.21047\nHelp | Advanced Search\nSearch\n\nGO\nquick links\n\nLogin\nHelp Pages\nAbout\n\nComputer Science > Machine Learning\narXiv:2503.21047 (cs)\n[Submitted on 26 Mar 2025]\nTitle:World Model Agents with Change-Based Intrinsic Motivation\nAuthors:Jeremias Ferrao, Rafael Cunha\nView a PDF of the paper titled World Model Agents with Change-Based Intrinsic Motivation, by Jeremias Ferrao and Rafael Cunha\nView PDFHTML (experimental)\n\nAbstract:Sparse reward environments pose a significant challenge for reinforcement learning due to the scarcity of feedback. Intrinsic motivation and transfer learning have emerged as promising strategies to address this issue. Change Based Exploration Transfer (CBET), a technique that combines these two approaches for model-free algorithms, has shown potential in addressing sparse feedback but its effectiveness with modern algorithms remains understudied. This paper provides an adaptation of CBET for world model algorithms like DreamerV3 and compares the performance of DreamerV3 and IMPALA agents, both with and without CBET, in the sparse reward environments of Crafter and Minigrid. Our tabula rasa results highlight the possibility of CBET improving DreamerV3's returns in Crafter but the algorithm attains a suboptimal policy in Minigrid with CBET further reducing returns. In the same vein, our transfer learning experiments show that pre-training DreamerV3 with intrinsic rewards does not immediately lead to a policy that maximizes extrinsic rewards in Minigrid. Overall, our results suggest that CBET provides a positive impact on DreamerV3 in more complex environments like Crafter but may be detrimental in environments like Minigrid. In the latter case, the behaviours promoted by CBET in DreamerV3 may not align with the task objectives of the environment, leading to reduced returns and suboptimal policies.\n\nComments:Submitted to Northern Lights Deep Learning Conference 2025\nSubjects:Machine Learning (cs.LG)\nCite as:arXiv:2503.21047 [cs.LG]\n(or arXiv:2503.21047v1 [cs.LG] for this version)\n\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Jeremias Lino Ferrao [view email]\n[v1] Wed, 26 Mar 2025 23:40:03 UTC (5,487 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled World Model Agents with Change-Based Intrinsic Motivation, by Jeremias Ferrao and Rafael Cunha\n\nView PDF\nHTML (experimental)\nTeX Source\n\nview license\nCurrent browse context:\ncs.LG\n<prev | next>\nnew | recent | 2025-03\nChange to browse by:\ncs\nReferences & Citations\n\nNASA ADS\nGoogle Scholar\nSemantic Scholar\n\nexport BibTeX citation Loading...\nBibTeX formatted citation\n\u00d7\nData provided by: \nBookmark\n\nBibliographic Tools\nBibliographic and Citation Tools\n\n[x] Bibliographic Explorer Toggle\n\nBibliographic Explorer (What is the Explorer?)\n\n[x] Connected Papers Toggle\n\nConnected Papers (What is Connected Papers?)\n\n[x] Litmaps Toggle\n\nLitmaps (What is Litmaps?)\n\n[x] scite.ai Toggle\n\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nCode, Data and Media Associated with this Article\n\n[x] alphaXiv Toggle\n\nalphaXiv (What is alphaXiv?)\n\n[x] Links to Code Toggle\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\n[x] DagsHub Toggle\n\nDagsHub (What is DagsHub?)\n\n[x] GotitPub Toggle\n\nGotit.pub (What is GotitPub?)\n\n[x] Huggingface Toggle\n\nHugging Face (What is Huggingface?)\n\n[x] Links to Code Toggle\n\nPapers with Code (What is Papers with Code?)\n\n[x] ScienceCast Toggle\n\nScienceCast (What is ScienceCast?)\nDemos\nDemos\n\n[x] Replicate Toggle\n\nReplicate (What is Replicate?)\n\n[x] Spaces Toggle\n\nHugging Face Spaces (What is Spaces?)\n\n[x] Spaces Toggle\n\nTXYZ.AI (What is TXYZ.AI?)\nRelated Papers\nRecommenders and Search Tools\n\n[x] Link to Influence Flower\n\nInfluence Flower (What are Influence Flowers?)\n\n[x] Core recommender toggle\n\nCORE Recommender (What is CORE?)\n\n[x] IArxiv recommender toggle\n\nIArxiv Recommender (What is IArxiv?)\n\nAuthor\nVenue\nInstitution\nTopic\n\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax) (What is MathJax?)\n\nAbout\n\nHelp\n\nContact\n\nSubscribe\n\nCopyright\n\nPrivacy Policy\n\nWeb Accessibility Assistance\n\narXiv Operational Status", "images": []}], "failed_results": [], "response_time": 1.83, "request_id": "b9046357-b30a-4ee4-9ea0-d7d396a6528b"}}
{"https://arxiv.org/abs/2010.11929": {"results": [{"url": "https://arxiv.org/abs/2010.11929", "title": "Transformers for Image Recognition at Scale - arXiv", "raw_content": "We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\n\ncs > arXiv:2010.11929\n\nComputer Science > Computer Vision and Pattern Recognition\narXiv:2010.11929 (cs)\n[Submitted on 22 Oct 2020 (v1), last revised 3 Jun 2021 (this version, v2)]\nTitle:An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\nAuthors:Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby\nView a PDF of the paper titled An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, by Alexey Dosovitskiy and 10 other authors\nView PDF\n\nAbstract:While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\n\nComments:\nFine-tuning code and pre-trained models are available at this https URL. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)\n\nSubjects:\nComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)\n\nCite as:\narXiv:2010.11929 [cs.CV]\n\n(or  arXiv:2010.11929v2 [cs.CV] for this version)\n\n arXiv-issued DOI via DataCite\n\nSubmission history\nFrom: Alexey Dosovitskiy [view email]\n[v1] Thu, 22 Oct 2020 17:55:59 UTC (3,194 KB)\n[v2] Thu, 3 Jun 2021 13:08:56 UTC (3,033 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, by Alexey Dosovitskiy and 10 other authors\n\nView PDF\nTeX Source\n\nview license\nCurrent browse context:\ncs.CV\n< prev    |    next >\nnew  |  recent  | 2020-10\nChange to browse by:\ncs\ncs.AI\ncs.LG\nReferences & Citations\n\nNASA ADS\nGoogle Scholar\nSemantic Scholar\n\n31 blog links\n(what is this?)\nDBLP - CS Bibliography\nlisting | bibtex\nAlexey Dosovitskiy\nLucas Beyer\nAlexander Kolesnikov\nDirk Weissenborn\nXiaohua Zhai\n\u2026\nexport BibTeX citation Loading...\nBibTeX formatted citation\n\u00d7\nData provided by:\nBookmark\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nReplicate (What is Replicate?)\nHugging Face Spaces (What is Spaces?)\nTXYZ.AI (What is TXYZ.AI?)\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\n\nAuthor\nVenue\nInstitution\nTopic\n\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax) (What is MathJax?)", "images": []}], "failed_results": [], "response_time": 0.02, "request_id": "8b8cef5a-6d7f-4211-82a2-a90d3fa0736b"}, "https://arxiv.org/abs/2002.05709": {"results": [{"url": "https://arxiv.org/abs/2002.05709", "title": "A Simple Framework for Contrastive Learning of Visual ... - arXiv", "raw_content": "We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\n\ncs > arXiv:2002.05709\n\nComputer Science > Machine Learning\narXiv:2002.05709 (cs)\n[Submitted on 13 Feb 2020 (v1), last revised 1 Jul 2020 (this version, v3)]\nTitle:A Simple Framework for Contrastive Learning of Visual Representations\nAuthors:Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton\nView a PDF of the paper titled A Simple Framework for Contrastive Learning of Visual Representations, by Ting Chen and 3 other authors\nView PDF\n\nAbstract:This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.\n\nComments:\nICML'2020. Code and pretrained models at this https URL\n\nSubjects:\nMachine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)\n\nCite as:\narXiv:2002.05709 [cs.LG]\n\n(or  arXiv:2002.05709v3 [cs.LG] for this version)\n\n arXiv-issued DOI via DataCite\n\nSubmission history\nFrom: Ting Chen [view email]\n[v1] Thu, 13 Feb 2020 18:50:45 UTC (5,093 KB)\n[v2] Mon, 30 Mar 2020 15:32:51 UTC (5,047 KB)\n[v3] Wed, 1 Jul 2020 00:09:08 UTC (5,829 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled A Simple Framework for Contrastive Learning of Visual Representations, by Ting Chen and 3 other authors\n\nView PDF\nTeX Source\n\nview license\nCurrent browse context:\ncs.LG\n< prev    |    next >\nnew  |  recent  | 2020-02\nChange to browse by:\ncs\ncs.CV\nstat\nstat.ML\nReferences & Citations\n\nNASA ADS\nGoogle Scholar\nSemantic Scholar\n\n15 blog links\n(what is this?)\nDBLP - CS Bibliography\nlisting | bibtex\nTing Chen\nSimon Kornblith\nMohammad Norouzi\nGeoffrey E. Hinton\nexport BibTeX citation Loading...\nBibTeX formatted citation\n\u00d7\nData provided by:\nBookmark\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nReplicate (What is Replicate?)\nHugging Face Spaces (What is Spaces?)\nTXYZ.AI (What is TXYZ.AI?)\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\nIArxiv Recommender (What is IArxiv?)\n\nAuthor\nVenue\nInstitution\nTopic\n\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax) (What is MathJax?)", "images": []}], "failed_results": [], "response_time": 0.02, "request_id": "5ee1c4e4-ed69-4a41-aca1-cda6b667216e"}, "https://arxiv.org/abs/2104.14294": {"results": [{"url": "https://arxiv.org/abs/2104.14294", "title": "[2104.14294] Emerging Properties in Self-Supervised Vision Transformers", "raw_content": "We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\n\ncs > arXiv:2104.14294\n\nComputer Science > Computer Vision and Pattern Recognition\narXiv:2104.14294 (cs)\n[Submitted on 29 Apr 2021 (v1), last revised 24 May 2021 (this version, v2)]\nTitle:Emerging Properties in Self-Supervised Vision Transformers\nAuthors:Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin\nView a PDF of the paper titled Emerging Properties in Self-Supervised Vision Transformers, by Mathilde Caron and 6 other authors\nView PDF\n\nAbstract:In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.\n\nComments:\n21 pages\n\nSubjects:\nComputer Vision and Pattern Recognition (cs.CV)\n\nCite as:\narXiv:2104.14294 [cs.CV]\n\n(or  arXiv:2104.14294v2 [cs.CV] for this version)\n\n arXiv-issued DOI via DataCite\n\nSubmission history\nFrom: Mathilde Caron [view email]\n[v1] Thu, 29 Apr 2021 12:28:51 UTC (30,222 KB)\n[v2] Mon, 24 May 2021 17:49:18 UTC (30,222 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled Emerging Properties in Self-Supervised Vision Transformers, by Mathilde Caron and 6 other authors\n\nView PDF\nTeX Source\n\nview license\nCurrent browse context:\ncs.CV\n< prev    |    next >\nnew  |  recent  | 2021-04\nChange to browse by:\ncs\nReferences & Citations\n\nNASA ADS\nGoogle Scholar\nSemantic Scholar\n\n4 blog links\n(what is this?)\nDBLP - CS Bibliography\nlisting | bibtex\nMathilde Caron\nHugo Touvron\nIshan Misra\nHerv\u00e9 J\u00e9gou\nJulien Mairal\n\u2026\nexport BibTeX citation Loading...\nBibTeX formatted citation\n\u00d7\nData provided by:\nBookmark\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nReplicate (What is Replicate?)\nHugging Face Spaces (What is Spaces?)\nTXYZ.AI (What is TXYZ.AI?)\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\n\nAuthor\nVenue\nInstitution\nTopic\n\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax) (What is MathJax?)", "images": []}], "failed_results": [], "response_time": 0.02, "request_id": "ffccc8d1-9a7a-4fde-b6bb-5d9b91d99da9"}}